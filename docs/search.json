[
  {
    "objectID": "intro/organizers.html",
    "href": "intro/organizers.html",
    "title": "Our Team",
    "section": "",
    "text": "See in English"
  },
  {
    "objectID": "intro/organizers.html#main-instructors-and-support",
    "href": "intro/organizers.html#main-instructors-and-support",
    "title": "Our Team",
    "section": "Main Instructors and Support",
    "text": "Main Instructors and Support\nThis effort was supported by the whole Hugging Face KREW team, with the following main folks listed alphabetically.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChanran Kim\nGabriel Yang\nHyunseo Yun\nKihoon Son\nNayeon Han\nSohyun Sim\nWonhyeong Seo\nWoojun Jung\n\n\n\n\nImage\n\n\n\n\n\n\n\n\n\n\nLinks"
  },
  {
    "objectID": "intro/organizers.html#hackathon-helpers",
    "href": "intro/organizers.html#hackathon-helpers",
    "title": "Our Team",
    "section": "Hackathon Helpers",
    "text": "Hackathon Helpers\n\nArthur Zucker, Hugging Face\nLysandre Debut, Hugging Face"
  },
  {
    "objectID": "submit.html",
    "href": "submit.html",
    "title": "제출하기",
    "section": "",
    "text": "See in English"
  },
  {
    "objectID": "submit.html#제출-가이드라인",
    "href": "submit.html#제출-가이드라인",
    "title": "제출하기",
    "section": "제출 가이드라인",
    "text": "제출 가이드라인\n\n1. 제출 기간\n\n10월 20일부터 11월 4일 24:00까지 프로젝트를 제출할 수 있습니다. 마감일 이후에는 접수되지 않습니다.\n\n\n\n2. 프로젝트 형식\n\n모든 프로젝트는 Hugging Face Spaces에 올라와 있어야 합니다.\n프로젝트의 README.md에는 문제 설명, 솔루션, 사용된 기술 및 기타 관련 정보를 설명하는 포괄적인 개요가 포함되어야 합니다.\n\n\n\n3. 팀 정보\n\n모든 팀원의 이름과 역할을 명확하게 언급하세요.\n연락을 받을 수 있는 팀 리더를 지정하세요.\n\n\n\n4. 제출 방법\n\n제출 양식 작성하기\n\n프로젝트 이름: 프로젝트를 대표하는 이름을 선택하세요.\n프로젝트 URL: 프로젝트의 Hugging Face Space 링크를 입력하세요.\n간단한 설명: 프로젝트에 대한 간략한 개요(최대 200자)를 작성하세요.\n팀원: 모든 팀원과 역할을 나열하세요.\n연락처 이메일: 팀 리더의 이메일을 입력하세요.\n\n추가 파일 업로드(선택 사항)\n\n프로젝트를 보완하는 추가 리소스, 문서 또는 슬라이드가 있는 경우 제공된 인터페이스를 사용하여 업로드하세요.\n\n검토 및 제출\n\n모든 정보가 정확한지 확인하세요. URL이 제대로 작동하는지 다시 확인하세요.\n제출을 클릭하여 프로젝트 제출을 완료합니다."
  },
  {
    "objectID": "submit.html#제출-후",
    "href": "submit.html#제출-후",
    "title": "제출하기",
    "section": "제출 후",
    "text": "제출 후\n제출이 완료되면 확인 이메일을 받게 됩니다. 프로젝트는 검토를 거치게 되며, 최종 후보에 오른 팀에게는 다음 단계에 대한 알림이 발송됩니다.\n일상에서의 AI 해커톤에 참여해 주셔서 감사합니다! 여러분의 혁신적인 솔루션을 기대하며 모든 참가자에게 행운이 함께하기를 기원합니다. 제출하는 동안 문제가 발생하면 Discord #support 채널을 통해 문의해 주시기 바랍니다.\n\n 제출하기"
  },
  {
    "objectID": "presentations/02/index.html",
    "href": "presentations/02/index.html",
    "title": "성취기준 기반 세특 생성기",
    "section": "",
    "text": "See in English"
  },
  {
    "objectID": "presentations/03/index.html",
    "href": "presentations/03/index.html",
    "title": "Hugging Face Korea Theme",
    "section": "",
    "text": "See in English"
  },
  {
    "objectID": "presentations/00/data/HF-K23-Tutorial.html",
    "href": "presentations/00/data/HF-K23-Tutorial.html",
    "title": "Hugging Face Hackathon 2023 튜토리얼",
    "section": "",
    "text": "See in English\n작성자: 정우준 (karl7ung@gmail.com)"
  },
  {
    "objectID": "presentations/00/data/HF-K23-Tutorial.html#시작-전-준비",
    "href": "presentations/00/data/HF-K23-Tutorial.html#시작-전-준비",
    "title": "Hugging Face Hackathon 2023 튜토리얼",
    "section": "시작 전 준비",
    "text": "시작 전 준비\n\n!pip install -q transformers\n!pip install -q datasets\n!pip install accelerate -U\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/7.9 MB 54.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 103.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 85.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.0/295.0 kB 27.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 493.7/493.7 kB 9.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 14.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 10.6 MB/s eta 0:00:00\nCollecting accelerate\n  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 261.4/261.4 kB 5.2 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch&gt;=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (4.5.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (2023.6.0)\nRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (2.1.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-&gt;accelerate) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-&gt;accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch&gt;=1.10.0-&gt;accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (2023.7.22)\nRequirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch&gt;=1.10.0-&gt;accelerate) (1.3.0)\nInstalling collected packages: accelerate\nSuccessfully installed accelerate-0.24.1\n\n\n\nfrom collections import defaultdict, Counter\nimport json\n\nimport numpy as np\nimport torch\n\n# 토큰화한 결과를 보기 편하게 만드는 함수\ndef print_encoding(model_inputs, indent=4):\n    indent_str = \" \" * indent\n    print(\"{\")\n    for k, v in model_inputs.items():\n        print(indent_str + k + \":\")\n        print(indent_str + indent_str + str(v))\n    print(\"}\")"
  },
  {
    "objectID": "presentations/00/data/HF-K23-Tutorial.html#part-0.-전체-과정-미리보기",
    "href": "presentations/00/data/HF-K23-Tutorial.html#part-0.-전체-과정-미리보기",
    "title": "Hugging Face Hackathon 2023 튜토리얼",
    "section": "Part 0. 전체 과정 미리보기",
    "text": "Part 0. 전체 과정 미리보기\n\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# 토크나이저 불러오기\ntokenizer = AutoTokenizer.from_pretrained(\"WhitePeak/bert-base-cased-Korean-sentiment\")\n# 모델 불러오기\nmodel = AutoModelForSequenceClassification.from_pretrained(\"WhitePeak/bert-base-cased-Korean-sentiment\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 전체 과정 미리보기\ninputs = \"음.... usb 직접 연결해서 충전하는게 편한데.. 아쉽네요.. 방수 때문에 그런건가...\"\ntokenized_inputs = tokenizer(inputs, return_tensors=\"pt\")\noutputs = model(**tokenized_inputs)\n\nlabels = ['NEGATIVE', \"POSITIVE\"]\npredictions = torch.argmax(outputs.logits)\n\n\nprint(\"Input:\")\nprint(inputs)\nprint()\nprint(\"Tokenized Inputs:\")\nprint_encoding(tokenized_inputs)\nprint()\nprint(\"Model Outputs:\")\nprint(outputs)\nprint()\nprint(f\"The prediction is {labels[predictions]}\")\n\nInput:\n음.... usb 직접 연결해서 충전하는게 편한데.. 아쉽네요.. 방수 때문에 그런건가...\n\nTokenized Inputs:\n{\n    input_ids:\n        tensor([[   101,   9634,    119,    119,    119,    119,  19626,  10457,  67288,\n           9568,  74322,  70146,   9770,  16617,  12178,  14153,   9924,  11102,\n          28911,    119,    119,   9519, 119072,  77884,  48549,    119,    119,\n           9328,  15891,  20729,   8924,  56710,  71439,  11287,    119,    119,\n            119,    102]])\n    token_type_ids:\n        tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n    attention_mask:\n        tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n}\n\nModel Outputs:\nSequenceClassifierOutput(loss=None, logits=tensor([[ 2.5508, -2.3483]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\nThe prediction is NEGATIVE"
  },
  {
    "objectID": "presentations/00/data/HF-K23-Tutorial.html#part-1.-기본적인-허깅페이스-사용법",
    "href": "presentations/00/data/HF-K23-Tutorial.html#part-1.-기본적인-허깅페이스-사용법",
    "title": "Hugging Face Hackathon 2023 튜토리얼",
    "section": "Part 1. 기본적인 허깅페이스 사용법",
    "text": "Part 1. 기본적인 허깅페이스 사용법\n\n1.1 토크나이저(Tokenizers)\nBERT 같은 트랜스포머 모델은 원시 문자열을 입력으로 받지 못합니다. 그래서 문자열을 모델이 이해할 수 있는 형태로 바꿔주는 과정이 필요합니다.\n토크나이저는 텍스트를 토큰으로 나누고, 그 토큰을 정수에 매핑하는 역할을 합니다.\n\n# 토크나이저의 종류\nfrom transformers import DistilBertTokenizer, DistilBertTokenizerFast, AutoTokenizer\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")        # Python\nprint(tokenizer)\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-cased\")    # Rust\nprint(tokenizer)\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")              # 제일 편리한 방법 (Default: Fast)\nprint(tokenizer)\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistilBertTokenizer(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\nDistilBertTokenizerFast(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\nDistilBertTokenizerFast(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n\n\n\n# 토크나이저를 사용하는 법\ninput_str = \"Welcome to the Hugging Face Hackathon 2023!\"\ntokenized_inputs = tokenizer(input_str)\n\n\nprint(\"Vanilla Tokenization\")\nprint_encoding(tokenized_inputs)\nprint()\n\n# Input_id에 접근하는 방법 2가지\nprint(tokenized_inputs.input_ids)\nprint(tokenized_inputs[\"input_ids\"])\n\nVanilla Tokenization\n{\n    input_ids:\n        [101, 12050, 1106, 1103, 20164, 10932, 10289, 11679, 2158, 9779, 1320, 17881, 1495, 106, 102]\n    attention_mask:\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n}\n\n[101, 12050, 1106, 1103, 20164, 10932, 10289, 11679, 2158, 9779, 1320, 17881, 1495, 106, 102]\n[101, 12050, 1106, 1103, 20164, 10932, 10289, 11679, 2158, 9779, 1320, 17881, 1495, 106, 102]\n\n\n\ncls = [tokenizer.cls_token_id]\nsep = [tokenizer.sep_token_id]\n\n# 토큰화가 이루어지는 단계\ninput_tokens = tokenizer.tokenize(input_str)                # 1. 텍스트를 토큰으로 분리\ninput_ids = tokenizer.convert_tokens_to_ids(input_tokens)   # 2. 토큰을 input ID로 변환\ninput_ids_special_tokens = cls + input_ids + sep            # 3. 특수 토큰([CLS], [SEP]) 추가\n\ndecoded_str = tokenizer.decode(input_ids_special_tokens)    # 디코딩(Decoding)\n\nprint(\"start:                   \", input_str)\nprint(\"tokenize:                \", input_tokens)\nprint(\"convert_tokens_to_ids:   \", input_ids)\nprint(\"add special tokens:      \", input_ids_special_tokens)\nprint(\"=========================\")\nprint(\"decode:                  \", decoded_str)\n\nstart:                    Welcome to the Hugging Face Hackathon 2023!\ntokenize:                 ['Welcome', 'to', 'the', 'Hu', '##gging', 'Face', 'Ha', '##ck', '##ath', '##on', '202', '##3', '!']\nconvert_tokens_to_ids:    [12050, 1106, 1103, 20164, 10932, 10289, 11679, 2158, 9779, 1320, 17881, 1495, 106]\nadd special tokens:       [101, 12050, 1106, 1103, 20164, 10932, 10289, 11679, 2158, 9779, 1320, 17881, 1495, 106, 102]\n=========================\ndecode:                   [CLS] Welcome to the Hugging Face Hackathon 2023! [SEP]\n\n\n\n# Fast Tokenizer를 사용하는 경우, 다른 방법을 사용할 수도 있습니다!\ninputs = tokenizer._tokenizer.encode(input_str)\n\nprint(input_str)\nprint(\"=\"*50)\nprint(f\"Number of tokens: {len(inputs)}\")\nprint(f\"Ids: {inputs.ids}\")\nprint(f\"Tokens: {inputs.tokens}\")\nprint(f\"Special tokens mask: {inputs.special_tokens_mask}\")\nprint()\nprint(\"char_to_token는 입력의 글자가 어떤 wordpiece에 있는지 알려줍니다\")\nchar_idx = 15\nprint(f\"예를 들어, 입력의 {char_idx + 1}번째 글자는 '{input_str[char_idx]}'이고, {inputs.char_to_token(char_idx)}번째 wordpiece인 {inputs.tokens[inputs.char_to_token(char_idx)]}의 일부입니다.\")\n\nWelcome to the Hugging Face Hackathon 2023!\n==================================================\nNumber of tokens: 15\nIds: [101, 12050, 1106, 1103, 20164, 10932, 10289, 11679, 2158, 9779, 1320, 17881, 1495, 106, 102]\nTokens: ['[CLS]', 'Welcome', 'to', 'the', 'Hu', '##gging', 'Face', 'Ha', '##ck', '##ath', '##on', '202', '##3', '!', '[SEP]']\nSpecial tokens mask: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n\nchar_to_token는 입력의 글자가 어떤 wordpiece에 있는지 알려줍니다\n예를 들어, 입력의 16번째 글자는 'H'이고, 4번째 wordpiece인 Hu의 일부입니다.\n\n\n\n# 유용한 기술들\n# 토크나이저는 PyTorch tensor를 리턴할 수 있습니다!\nmodel_inputs = tokenizer(\"Hugging Face Transformers is cool!\", return_tensors='pt') # Tensorflow 나 Jax도 가능합니다!\nprint(\"PyTorch Tensors:\")\nprint_encoding(model_inputs)\n\nPyTorch Tensors:\n{\n    input_ids:\n        tensor([[  101, 20164, 10932, 10289, 25267,  1110,  4348,   106,   102]])\n    attention_mask:\n        tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])\n}\n\n\n\n# 여러 문장을 한 번에 토크나이즈하고 패딩할 수 있습니다\nmodel_inputs = tokenizer([\"Hugging Face Transformers is cool!\",\n                          \"The quick brown fox jumps over the lazy dog. Then the do got up and ran away because she didn't like foxes.\"],\n                         return_tensors='pt',\n                         padding=True,\n                         truncation=True)\nprint(f\"Pad token: {tokenizer.pad_token} | Pad token id: {tokenizer.pad_token_id}\")\nprint(\"Padding:\")\nprint_encoding(model_inputs)\n\nPad token: [PAD] | Pad token id: 0\nPadding:\n{\n    input_ids:\n        tensor([[  101, 20164, 10932, 10289, 25267,  1110,  4348,   106,   102,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [  101,  1109,  3613,  3058, 17594, 15457,  1166,  1103, 16688,  3676,\n           119,  1599,  1103,  1202,  1400,  1146,  1105,  1868,  1283,  1272,\n          1131,  1238,   112,   189,  1176, 17594,  1279,   119,   102]])\n    attention_mask:\n        tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1]])\n}\n\n\n\n# 마찬가지로 여러 문장을 한 번에 디코딩할 수 있습니다\nprint(\"Batch Decode:\")\nprint(tokenizer.batch_decode(model_inputs.input_ids))\nprint()\nprint(\"Batch Decode: (no special characters)\")\nprint(tokenizer.batch_decode(model_inputs.input_ids, skip_special_tokens=True))\n\nBatch Decode:\n['[CLS] Hugging Face Transformers is cool! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', \"[CLS] The quick brown fox jumps over the lazy dog. Then the do got up and ran away because she didn't like foxes. [SEP]\"]\n\nBatch Decode: (no special characters)\n['Hugging Face Transformers is cool!', \"The quick brown fox jumps over the lazy dog. Then the do got up and ran away because she didn't like foxes.\"]\n\n\n\n상품 리뷰 감정 분류 예시\n\n# 쇼핑 감성 분석 예제\n\n# 토크나이저 불러오기\ntokenizer = AutoTokenizer.from_pretrained(\"WhitePeak/bert-base-cased-Korean-sentiment\")\n\ninput_str = \"음.... usb 직접 연결해서 충전하는게 편한데.. 아쉽네요.. 방수 때문에 그런건가...\"\n\nprint(\"Tokenization: \")\ntokenized_input = tokenizer(input_str)\nprint_encoding(tokenized_input)\nprint()\n\ndecoded_str = tokenizer.decode(tokenized_input.input_ids)\nprint(\"Decode:\", decoded_str)\n\nTokenization: \n{\n    input_ids:\n        [101, 9634, 119, 119, 119, 119, 19626, 10457, 67288, 9568, 74322, 70146, 9770, 16617, 12178, 14153, 9924, 11102, 28911, 119, 119, 9519, 119072, 77884, 48549, 119, 119, 9328, 15891, 20729, 8924, 56710, 71439, 11287, 119, 119, 119, 102]\n    token_type_ids:\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    attention_mask:\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n}\n\nDecode: [CLS] 음.... usb 직접 연결해서 충전하는게 편한데.. 아쉽네요.. 방수 때문에 그런건가... [SEP]\n\n\n\n\n\n1.2 모델(Models)\n허깅페이스의 큰 장점 중 하나는 다른 사람이나 기업이 올린 모델을 쉽게 가져다 쓸 수 있다는 것입니다!\n모델을 그냥 사용할 수도 있고, 자신이 필요한 태스크에 따라서 헤드를 붙여서 사용할 수도 있습니다.\n헤드의 종류는 아래와 같습니다.\n*\n*ForMaskedLM\n*ForSequenceClassification\n*ForTokenClassification\n*ForQuestionAnswering\n*ForMultipleChoice\n...\nhttps://huggingface.co/docs/transformers/model_doc/auto\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n\ninput_str = \"Hugging Face Transformers is cool!\"\n\nprint(input_str)\nprint(\"=\"*50)\nmodel_inputs = tokenizer(input_str, return_tensors='pt')\n\nHugging Face Transformers is cool!\n==================================================\n\n\n\nfrom transformers import AutoModelForSequenceClassification, DistilBertForSequenceClassification\n\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\nmodel = AutoModelForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n\n\n\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nSequence Classification 태스크에 대한 파라미터는 아직 학습되지 않았기 때문에 위와 같은 경고가 뜹니다.\n\n\n# 방법 1\nmodel_outputs = model(input_ids=model_inputs.input_ids, attention_mask=model_inputs.attention_mask)\n\n# 방법 2\nmodel_outputs = model(**model_inputs)\n\nprint(model_inputs)\nprint()\nprint(model_outputs)\nprint()\n\nmodel_inputs = tokenizer(input_str, return_tensors='pt')\n\nlabels = [\"NEGATIVE\", \"POSITIVE\"]\nmodel_inputs['labels'] = torch.tensor([1])\n\nmodel_outputs = model(**model_inputs)\n\n\nprint(model_outputs)\nprint()\nprint(f\"Model predictions: {labels[model_outputs.logits.argmax()]}\")\n\n{'input_ids': tensor([[  101, 20164, 10932, 10289, 25267,  1110,  4348,   106,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.0369,  0.0166]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\nSequenceClassifierOutput(loss=tensor(0.6667, grad_fn=&lt;NllLossBackward0&gt;), logits=tensor([[-0.0369,  0.0166]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\nModel predictions: POSITIVE\n\n\n\n# 토크나이저 불러오기\ntokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\")\n# 모델 불러오기\nmodel = AutoModelForSequenceClassification.from_pretrained(\"monologg/kobert\")\n\ninput_str = \"조용한건 좋은데 냄새가 너무너무 납니다. 환기를 해도 냄새가 안빠지는데 몸에는 무해할지 걱정이네요.\"\nmodel_inputs = tokenizer(input_str, return_tensors='pt')\nmodel_outputs = model(**model_inputs)\n\nprint(model_outputs)\nprint(f\"Model predictions: {labels[model_outputs.logits.argmax()]}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.2253, -0.2035]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\nModel predictions: POSITIVE\n\n\n\n상품 리뷰 감정 분류 예시\n\n# 토크나이저 불러오기\ntokenizer = AutoTokenizer.from_pretrained(\"WhitePeak/bert-base-cased-Korean-sentiment\")\n# 모델 불러오기\nmodel = AutoModelForSequenceClassification.from_pretrained(\"WhitePeak/bert-base-cased-Korean-sentiment\")\n\ninput_str = \"조용한건 좋은데 냄새가 너무너무 납니다. 환기를 해도 냄새가 안빠지는데 몸에는 무해할지 걱정이네요.\"\nmodel_inputs = tokenizer(input_str, return_tensors='pt')\nmodel_outputs = model(**model_inputs)\n\nprint(model_outputs)\nprint(f\"Model predictions: {labels[model_outputs.logits.argmax()]}\")\n\nSequenceClassifierOutput(loss=None, logits=tensor([[ 1.8190, -1.8091]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\nModel predictions: NEGATIVE"
  },
  {
    "objectID": "presentations/00/data/HF-K23-Tutorial.html#part-2.-파인튜닝fine-tuning",
    "href": "presentations/00/data/HF-K23-Tutorial.html#part-2.-파인튜닝fine-tuning",
    "title": "Hugging Face Hackathon 2023 튜토리얼",
    "section": "Part 2. 파인튜닝(Fine-tuning)",
    "text": "Part 2. 파인튜닝(Fine-tuning)\nhttps://huggingface.co/datasets\n\n2.1 데이터셋 불러오기\n\nfrom datasets import load_dataset, DatasetDict\n\namazon_dataset = load_dataset(\"KETI-AIR/kor_amazon_polarity\")\n\ndef truncate(example):\n    return {\n        \"content\": \" \".join(example['content'].split()[:20]),\n        \"label\": example['label']\n    }\n\nsmall_amazon_dataset = DatasetDict(\n    train=amazon_dataset['train'].shuffle(seed=777).select(range(128)).map(truncate),\n    val=amazon_dataset['train'].shuffle(seed=777).select(range(128, 160)).map(truncate),\n)\n\nsmall_amazon_dataset['train'][:10]\n\n\n\n\n\n\n\n{'label': [0, 0, 1, 0, 1, 1, 0, 0, 1, 1],\n 'title': ['작은 남자의 촌스러운 작은 책',\n  '무슨 일이야, 프랭크?',\n  'BIC American DV62si 스피커',\n  ' Zero STARS',\n  '가격 대비 고급 커버',\n  'SUG GLOVE 좋은 작업용 장갑',\n  '흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐',\n  '나는 내가 바라는 것을 얻었다.',\n  '훌륭한 의학적 및 행동적 조언',\n  '옛날으로 돌아가는 것 같은...'],\n 'content': ['스테레오타입적 사고(말할 것도 없이 빈약한 글쓰기)에서 이 이해할 수 있는 운동을 읽는 내 인생의 세 시간을 낭비한 후, 존 그레이가',\n  '나는 궁금하다. 맥코트는 어떻게 그렇게 빨리 패스트볼을 잃었을까?이 속편은 대부분의 속편과 마찬가지로 평평합니다.나는 우리 대부분의 필사적인 사람들이 특정한 양의 광채와',\n  '이것들은 작은 방에 필요한 훌륭한 소형 스피커입니다.나는 내가 연주하는 음악에서 놀라운 선명도를 얻지만, 스피커가 더 광범위한 시스템에 적합하지 않다고 생각합니다.시스템이',\n  '이것은 현재의 인간 상태에 영향을 미치는 DNA 없이 우리의 병든 세계가 할 수 있는 바로 그 종류의 \"엔터테인먼트\"이다.',\n  '가방은 나일론으로 만들어졌으며 보관을 위한 일치하는 가방이 함께 제공됩니다.표준 캠핑 텐트와 유사한 소재로 제작되었습니다.예산 충당을 위해 확실히 그만한 가치가 있습니다.',\n  '남편은 사랑하는 수그 장갑 한 레를 가지고 있었고, 교체가 필요할 정도로 많이 사용했습니다.원래 구매한 매장에서 찾을 수 없어서 아마존.com에서 주문할',\n  '여성들은 아름답고 그것만이 긍정적이에요.줄거리는 지루하고 대사는 끔찍하다.디자이너 빅토리 포드는 가장 추한 옷을 만든다.니코 라일리는 가짜로 만난다.웬디 힐리, 진짜?그녀는 기업 임원이고',\n  '전에는 대부분의 노래를 들었지만 LP에서 들었어요.CD는 다루기 쉽고 더 나은 사운드를 제공합니다.',\n  '건강과 응급 처치에 대한 훌륭한 조언 외에도 아이들이 다양한 연령대에서 발달적으로 겪고 있는 일에 대한 좋은 설명이 있습니다.이것들은 우리가 수면',\n  '토미 라이브를 본 사람이라면 누구를 위한 것인가.어쨌든 그 환상적인 날들과 일치하는 것은 아닙니다.그러나 눈을 감으면 토미가 그 무대에서 술집으로 올라가'],\n 'data_index_by_user': [2401552,\n  677713,\n  1966471,\n  504948,\n  2398284,\n  3176101,\n  1792091,\n  647749,\n  3058703,\n  1005585]}\n\n\n\n\n\nsmall_amazon_dataset = small_amazon_dataset.remove_columns([\"title\", 'data_index_by_user'])\nsmall_amazon_dataset[\"train\"][:10]\n\n{'label': [0, 0, 1, 0, 1, 1, 0, 0, 1, 1],\n 'content': ['스테레오타입적 사고(말할 것도 없이 빈약한 글쓰기)에서 이 이해할 수 있는 운동을 읽는 내 인생의 세 시간을 낭비한 후, 존 그레이가',\n  '나는 궁금하다. 맥코트는 어떻게 그렇게 빨리 패스트볼을 잃었을까?이 속편은 대부분의 속편과 마찬가지로 평평합니다.나는 우리 대부분의 필사적인 사람들이 특정한 양의 광채와',\n  '이것들은 작은 방에 필요한 훌륭한 소형 스피커입니다.나는 내가 연주하는 음악에서 놀라운 선명도를 얻지만, 스피커가 더 광범위한 시스템에 적합하지 않다고 생각합니다.시스템이',\n  '이것은 현재의 인간 상태에 영향을 미치는 DNA 없이 우리의 병든 세계가 할 수 있는 바로 그 종류의 \"엔터테인먼트\"이다.',\n  '가방은 나일론으로 만들어졌으며 보관을 위한 일치하는 가방이 함께 제공됩니다.표준 캠핑 텐트와 유사한 소재로 제작되었습니다.예산 충당을 위해 확실히 그만한 가치가 있습니다.',\n  '남편은 사랑하는 수그 장갑 한 레를 가지고 있었고, 교체가 필요할 정도로 많이 사용했습니다.원래 구매한 매장에서 찾을 수 없어서 아마존.com에서 주문할',\n  '여성들은 아름답고 그것만이 긍정적이에요.줄거리는 지루하고 대사는 끔찍하다.디자이너 빅토리 포드는 가장 추한 옷을 만든다.니코 라일리는 가짜로 만난다.웬디 힐리, 진짜?그녀는 기업 임원이고',\n  '전에는 대부분의 노래를 들었지만 LP에서 들었어요.CD는 다루기 쉽고 더 나은 사운드를 제공합니다.',\n  '건강과 응급 처치에 대한 훌륭한 조언 외에도 아이들이 다양한 연령대에서 발달적으로 겪고 있는 일에 대한 좋은 설명이 있습니다.이것들은 우리가 수면',\n  '토미 라이브를 본 사람이라면 누구를 위한 것인가.어쨌든 그 환상적인 날들과 일치하는 것은 아닙니다.그러나 눈을 감으면 토미가 그 무대에서 술집으로 올라가']}\n\n\n\n# 데이터셋 준비 - 여기선 16개의 예제를 하나의 배치로 토큰화하겠습니다\ntokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\")\n\nsmall_tokenized_dataset = small_amazon_dataset.map(\n    lambda example: tokenizer(example['content'], padding=True, truncation=True),\n    batched=True,\n    batch_size=16\n)\n\nsmall_tokenized_dataset = small_tokenized_dataset.remove_columns([\"content\", \"token_type_ids\"])\nsmall_tokenized_dataset = small_tokenized_dataset.rename_column(\"label\", \"labels\")\nsmall_tokenized_dataset.set_format(\"torch\")\n\n\n\n\n\n\n\n\nsmall_tokenized_dataset['train'][0:2]\n\n{'labels': tensor([0, 0]),\n 'input_ids': tensor([[   2,    0, 6496,   18,    0,    0, 6883,    0,    0,   40, 6903, 7096,\n             0, 6629, 7142,    0,    0, 5678,    0, 6579,    0,    0, 7968,   46,\n          7264,    0,    3,    1,    1,    1,    1,    1,    1],\n         [   2, 5658,    0,   54,    0,    0,    0,    0,    0,    0,  258, 7096,\n             0,    0,    0,    0,    0,   54, 5658, 7007,    0,    0,    0,    0,\n             0,    0,    3,    1,    1,    1,    1,    1,    1]]),\n 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n\n\n\nfrom torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(small_tokenized_dataset['train'], batch_size=16)\neval_dataloader = DataLoader(small_tokenized_dataset['val'], batch_size=16)\n\n\n\n2.2 학습하기\n\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm.notebook import tqdm\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained('monologg/kobert', num_labels=2)\n\nnum_epochs = 3\nnum_training_steps = 3 * len(train_dataloader)\noptimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\nlr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\nbest_val_loss = float(\"inf\")\nprogress_bar = tqdm(range(num_training_steps))\nfor epoch in range(num_epochs):\n    # 학습(Training)\n    model.train()\n    for batch_i, batch in enumerate(train_dataloader):\n\n        output = model(**batch)\n\n        optimizer.zero_grad()\n        output.loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        progress_bar.update(1)\n\n    # 평가(Validation)\n    model.eval()\n    loss = 0\n    for batch_i, batch in enumerate(eval_dataloader):\n        with torch.no_grad():\n            output = model(**batch)\n        loss += output.loss\n\n    avg_val_loss = loss / len(eval_dataloader)\n    print(f\"Validation loss: {avg_val_loss}\")\n    if avg_val_loss &lt; best_val_loss:\n        print(\"Saving checkpoint!\")\n        best_val_loss = avg_val_loss\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': best_val_loss,\n            },\n            f\"./epoch_{epoch}.pt\"\n        )\n\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\n\n\n\n\nValidation loss: 0.6929049491882324\nSaving checkpoint!\nValidation loss: 0.6971858143806458\nValidation loss: 0.6970363259315491\n\n\n\nfrom transformers import TrainingArguments, Trainer\n\ntokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\")\nmodel = AutoModelForSequenceClassification.from_pretrained('monologg/kobert', num_labels=2)\n\narguments = TrainingArguments(\n    output_dir=\"sample_hf_trainer\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    load_best_model_at_end=True,\n    seed=224,\n    logging_steps=5\n)\n\n\ndef compute_metrics(eval_pred):\n    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return {\"accuracy\": np.mean(predictions == labels)}\n\n\ntrainer = Trainer(\n    model=model,\n    args=arguments,\n    train_dataset=small_tokenized_dataset['train'],\n    eval_dataset=small_tokenized_dataset['val'],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n# 모델 훈련\ntrainer.train()\n\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n\n    \n      \n      \n      [24/24 00:23, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n0.732800\n0.678418\n0.593750\n\n\n2\n0.682300\n0.681364\n0.593750\n\n\n3\n0.661700\n0.680216\n0.593750\n\n\n\n\n\n\nTrainOutput(global_step=24, training_loss=0.6973832249641418, metrics={'train_runtime': 27.7682, 'train_samples_per_second': 13.829, 'train_steps_per_second': 0.864, 'total_flos': 7597331723520.0, 'train_loss': 0.6973832249641418, 'epoch': 3.0})\n\n\n\n# evaluating the model is very easy\n\n# results = trainer.evaluate()                           # just gets evaluation metrics\nresults = trainer.predict(small_tokenized_dataset['val']) # also gives you predictions\n\n\n\n\n\nresults\n\nPredictionOutput(predictions=array([[ 0.1916449 ,  0.22683379],\n       [ 0.22314717, -0.11641365],\n       [ 0.20360216, -0.09543565],\n       [ 0.19376771, -0.11019652],\n       [ 0.22164695, -0.1203385 ],\n       [ 0.15413123, -0.14183237],\n       [ 0.1821831 , -0.11664201],\n       [ 0.16595587, -0.13742277],\n       [ 0.18666586, -0.08931123],\n       [ 0.1758633 , -0.13222699],\n       [ 0.18192168, -0.11654314],\n       [ 0.20246285, -0.11635383],\n       [ 0.17683929, -0.10863604],\n       [ 0.2183371 , -0.10548168],\n       [ 0.19470072, -0.09061436],\n       [ 0.17531334, -0.12730497],\n       [ 0.22123067, -0.08326611],\n       [ 0.15778846, -0.11511832],\n       [ 0.22013862, -0.0995794 ],\n       [ 0.19786586, -0.08956669],\n       [ 0.20206194,  0.24839233],\n       [ 0.18367225, -0.10238333],\n       [ 0.18084264, -0.14627627],\n       [ 0.19924533, -0.14783694],\n       [ 0.1901607 , -0.09713633],\n       [ 0.1657788 , -0.12214693],\n       [ 0.19704998, -0.11093342],\n       [ 0.16072392, -0.12247129],\n       [ 0.27469176,  0.29437575],\n       [ 0.2095184 , -0.09706718],\n       [ 0.1486406 , -0.15522346],\n       [ 0.20539631, -0.084805  ]], dtype=float32), label_ids=array([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n       0, 0, 1, 0, 0, 0, 1, 1, 0, 0]), metrics={'test_loss': 0.6784175634384155, 'test_accuracy': 0.59375, 'test_runtime': 0.1055, 'test_samples_per_second': 303.235, 'test_steps_per_second': 18.952})\n\n\n\n# To load our saved model, we can pass the path to the checkpoint into the `from_pretrained` method:\ntest_str = \"음.... usb 직접 연결해서 충전하는게 편한데.. 아쉽네요.. 방수 때문에 그런건가...\"\n\nfinetuned_model = AutoModelForSequenceClassification.from_pretrained(\"sample_hf_trainer/checkpoint-24\")\nmodel_inputs = tokenizer(test_str, return_tensors=\"pt\")\nprediction = torch.argmax(finetuned_model(input_ids=model_inputs.input_ids, attention_mask=model_inputs.attention_mask).logits)\nprint([\"NEGATIVE\", \"POSITIVE\"][prediction])\n\nPOSITIVE"
  },
  {
    "objectID": "presentations/00/data/HF-K23-Tutorial.html#part-3.-hugging-face에-모델-업로드",
    "href": "presentations/00/data/HF-K23-Tutorial.html#part-3.-hugging-face에-모델-업로드",
    "title": "Hugging Face Hackathon 2023 튜토리얼",
    "section": "Part 3. Hugging Face에 모델 업로드",
    "text": "Part 3. Hugging Face에 모델 업로드\n\n!pip install huggingface_hub\n\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.17.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\nRequirement already satisfied: packaging&gt;=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (2023.7.22)\n\n\n\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n\n\n\n\n\ntrainer.push_to_hub()\n\n\n\n\n\n\n\n\n\n\n\n\n\n'https://huggingface.co/jungnerd/sample_hf_trainer/tree/main/'"
  },
  {
    "objectID": "presentations/04/index.html",
    "href": "presentations/04/index.html",
    "title": "Colorful Illustration",
    "section": "",
    "text": "See in English"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "2023 일상에서의 AI 해커톤",
    "section": "",
    "text": "See in English"
  },
  {
    "objectID": "index.html#소개",
    "href": "index.html#소개",
    "title": "2023 일상에서의 AI 해커톤",
    "section": "소개",
    "text": "소개\n안녕하세요!\n본 해커톤은 Hugging Face를 사랑하는 모든 분들을 위한 행사로 여러분의 열정과 창의력을 마음껏 펼칠 수 있는 자리입니다. 아래 안내사항을 주의 깊게 읽어보시고, 이번 행사를 통해 함께 성장하는 시간을 가져보시기 바랍니다.\n\n1. 어떻게 진행하나요?\n해커톤에 참여하는 과정은 간단합니다! 아래 4단계를 차근차근 따라와 주세요:\n\n  \n    \n    \n    디스코드 참여\n  \n  \n    \n    \n    프로젝트 생성\n  \n  \n    \n    \n    기여 및 제출\n  \n  \n    \n    \n    수료와 그 이후\n    \n    (예정)\n  \n\n\n디스코드 참여: 해커톤의 소식을 접하고, 함께할 팀을 구성하거나 친구들을 만날 수 있는 디스코드 커뮤니티에 참여해보세요!\n프로젝트 생성: PseudoLab 조직에 프로젝트 데모를 업로드하고, 세계에 여러분의 창의력을 보여주세요!\n기여 및 제출: 프로젝트를 발전시키고, Hub에서 Pull Request를 통해 모두와 아이디어를 공유하며 같이 배워가세요. 여러분의 성장이 커뮤니티의 힘이 됩니다. 리더보드로 얼마나 인기 있는지도 확인할 수 있답니다!\n수료와 그 이후: 해커톤을 성공적으로 마친 후, 오프라인에서도 만나고 싶다면 이벤터스에서 팀 정보와 프로젝트 URL을 제출하고, 참가 확정 메일을 기다려주세요!\n\n\n\n\n\n\n\n\n타인의 작업을 무단으로 사용하는 경우 제출 작품은 무효 처리되며,\n프로젝트에 대한 설명과 데모는 성의 있게 작성해주셔야 합니다.\n\n\n\n\n\n\n2. 보상 및 혜택\n\nHF 크레딧 지원 (~11월 11일까지)\n해커톤 완주 수료증 (예시)\n다양한 네트워킹 기회\n오프라인 행사 초청자에게 제공되는 특별한 혜택 (상세 내용은 추후 공개 예정)\n\n\n\n3. 오프라인 행사\n신청자 중 “Like”를 많이 받은 프로젝트 기여자들을 PseudoCon에 초대하여 데모를 서로 공유하고 네트워킹할 수 있는 시간을 가집니다. 상세한 내용은 오프라인 행사 신청 페이지에서 확인하실 수 있습니다.\n\n\n4. 주요 일정\n\n\n\n일정\n날짜\n\n\n\n\n해커톤 시작\n10월 20일(금)\n\n\n수료증 발급 마감\n11월 10일(금) 24시\n\n\n개별 오프라인 초대\n11월 6일(월) ~ 11월 9일(목)\n\n\nHugging Face 로컬 밋업 w/ PseudoCon\n11월 11일 (토)\n\n\n\n\n\n\n5. 대회 목표\n일상에서의 AI 해커톤은\n\n사전 경험과 관계없이 모든 분들이 참여할 수 있고,\n서로 배우며 성장할 수 있는 커뮤니티를 만드는 것에\n\n큰 중점을 두고 있습니다. 제공된 krew23-hackathon 스레드에서 함께 창의적인 아이디어를 공유하고 실력을 향상시킬 수 있습니다.\n참가자 여러분의 많은 관심과 참여를 부탁드립니다. 함께 성장하고, 서로를 격려하는 커뮤니티를 만들어가요. 감사합니다."
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "2023 일상에서의 AI 해커톤",
    "section": "",
    "text": "See in English"
  },
  {
    "objectID": "tutorials/index.html#튜토리얼-개요",
    "href": "tutorials/index.html#튜토리얼-개요",
    "title": "2023 일상에서의 AI 해커톤",
    "section": "튜토리얼 개요",
    "text": "튜토리얼 개요\n안녕하세요, 참가자 여러분!\n여러분을 위한 여정의 첫 페이지에 오신 것을 환영합니다. 여기에서 여러분이 Hugging Face KREW 해커톤 2023에 참여하며 필요로 하는 기본적인 정보와 지침을 제공할 것입니다.\n프로젝트 시작부터 제출, 평가까지의 과정을 단계별로 간단히 안내드릴 예정이니, 해커톤을 준비하며 이 페이지를 참고 자료로 꼭 활용해 주세요!\n\n01. 시작하기 전에: 준비사항 확인\n\n소통 채널 참가하기\n팀 빌딩 가이드\n\n\n\n02. 프로젝트 생성: 첫 걸음 떼기\n\nHugging Face 가입 방법\nPseudo Lab 기여자 등록\n프로젝트 등록 및 설정\n\n\n\n03. 프로젝트 제출 및 평가: 결과물의 가치 높이기\n\n프로젝트에 기여하는 방법\n커뮤니티와의 소통 및 협업 가이드\n완성된 프로젝트의 제출\n\n\n\n04. 해커톤 마무리: 여정의 종착점 (22일 배포 예정)\n\n수료증 발급 절차\n참가자 피드백 및 소감 공유\n해커톤 후속 활동 및 기회\n\n이 커리큘럼은 여러분이 해커톤에서 성공적인 결과를 이끌어낼 수 있도록 설계되었습니다. 각 섹션을 꼼꼼히 읽고 지침을 따라주신다면, 더 많은 인사이트와 기회를 얻으실 수 있을 거예요.\n이번 해커톤이 여러분의 기술적 성장과 창의적 발전에 큰 기여가 되길 바랍니다. 함께 해커톤을 통한 특별한 경험을 만들어가요. 행운을 빕니다!"
  },
  {
    "objectID": "tutorials/pr_guide.html",
    "href": "tutorials/pr_guide.html",
    "title": "2023 일상에서의 AI 해커톤",
    "section": "",
    "text": "See in English"
  },
  {
    "objectID": "tutorials/pr_guide.html#기여-및-제출",
    "href": "tutorials/pr_guide.html#기여-및-제출",
    "title": "2023 일상에서의 AI 해커톤",
    "section": "03_기여 및 제출",
    "text": "03_기여 및 제출\n이제부터는 커뮤니티와의 소통과 평가 과정을 통해 프로젝트의 가치를 더욱 높일 수 있는 단계입니다. 해커톤은 서로의 작업을 공유하고 성장할 수 있는 소중한 기회이니만큼 적극적인 참여 부탁드려요!\n\n프로젝트에 기여하기\n먼저 흥미있는 프로젝트의 Community 탭을 방문해서 New Pull Request 버튼을 클릭하세요.\n\n여러분이 기여하고자 하는 내용을 명확히 전달하기 위해, 새 창에서 나타나는 커밋 메시지 부분에 계획하는 작업의 개요를 간단명료하게 작성해 주세요. 이렇게 하면 나중에 프로젝트 기여 내역을 파악하기가 훨씬 수월해집니다.\n\n화면에 보이는 것처럼 로컬 환경에서 작업할 수 있도록 clone, fetch, checkout을 해주세요.\n\n\n\n액세스 토큰 생성과 로그인 과정\n이어서, 여러분의 Hugging Face 계정으로 액세스 토큰을 만들어 로그인하는 단계가 필요합니다. 이 토큰은 여러분의 작업을 인증하고 보호하는 역할을 해요. 자세한 설명은 영문 가이드를 참조하시길 바랍니다.\n\n로그인이 제대로 되었는지 확인하려면 터미널에서 huggingface-cli whoami 명령어를 실행하시면 됩니다.\n\n\n작업 공유와 커밋\n프로젝트를 진행하며 중요한 단계마다 커밋을 하고, 이를 푸시해서 커뮤니티와 계속 소통하세요. 이 과정을 통해 여러분의 작업 진척 상황을 투명하게 공유할 수 있어요.\n\n\n\nREADME 작성하기\n다시 강조드리지만 프로젝트의 README 파일은 누구나 프로젝트를 쉽게 이해할 수 있도록 상세하게 작성해야 해요. 프로젝트의 목적, 사용된 기술, 주요 변경 사항 등을 포함하여, 이 프로젝트에 대한 충분한 정보를 제공해 주세요.\n\n\n해커톤 및 평가 일정 확인\n프로젝트 제출 이후, 전체 해커톤 기간인 3주 동안 여러분의 프로젝트는 커뮤니티의 리뷰와 평가를 받게 됩니다. 이 기간에 프로젝트의 질을 보증하고 부정행위를 방지하기 위해, 팀별로 적어도 (팀원 수 + 2)개의 ’좋아요’를 받아야 수료증이 발급된다는 점, 잊지 마세요!"
  },
  {
    "objectID": "tutorials/discord.html",
    "href": "tutorials/discord.html",
    "title": "2023 일상에서의 AI 해커톤",
    "section": "",
    "text": "See in English"
  },
  {
    "objectID": "tutorials/discord.html#디스코드-참여",
    "href": "tutorials/discord.html#디스코드-참여",
    "title": "2023 일상에서의 AI 해커톤",
    "section": "01_디스코드 참여",
    "text": "01_디스코드 참여\n해커톤에 오신 것을 환영합니다! 먼저 아래 링크를 통해 커뮤니티 서버에 오셔서 인사를 남겨주세요!\n\n\n\n\n\n\nhttps://hf.co/join/discord\n\n\n\n\n\n해커톤 채널이 안 보여요!\n서버에 처음 들어오셨다면, Open Source ML 역할을 추가하고 krew23-hackathon 채널에 참여하여 해커톤과 관련된 다양한 대화에 뛰어들 수 있습니다. 여러분의 참여가 이곳의 에너지와 창의성을 한층 더 끌어올릴 것입니다. \n\n\n첫 메시지는 무엇이 좋을까요?\n커뮤니티와의 첫 인사는 깊은 인상을 남기기에 충분합니다. krew23-hackathon 채널에서 자신을 소개하며 시작해보세요. 본인의 전문 분야, 해커톤에서 이루고자 하는 목표, 그리고 함께하고 싶은 팀원의 조건 등을 말해주시면 다른 참가자들이 여러분을 더 잘 이해할 수 있습니다. \n또한, 다른 참가자들의 스레드에 적극적으로 참여하세요. 아이디어에 당신의 피드백을 공유하고, 함께 프로젝트를 만들어나가는 과정에서 서로의 생각을 충분히 교환하세요.\n\n\n더 궁금하신 점이 있으신가요?\n언제든지 궁금한 점이 생기면 아래 운영진에게 문의하여 즉각적인 도움을 받을 수 있습니다.\n\n@wonhseo\n@gabriely1004\n@jungnerd\n\n이제, 여러분은 해커톤의 중심지인 이 커뮤니티의 일원입니다. 여러분의 참여와 소통이 이 커뮤니티를 더욱 풍요롭게 만든다는 점, 잊지 말아주세요. 여러분의 새로운 도전을 응원합니다!"
  },
  {
    "objectID": "tutorials/hf_hub.html",
    "href": "tutorials/hf_hub.html",
    "title": "02_프로젝트 생성",
    "section": "",
    "text": "See in English\n\n이제 본격적으로 해커톤 여정을 시작해봅시다! 이번 해커톤에서는 각자의 뛰어난 아이디어와 프로젝트를 Hugging Face KREW Hackathon 2023 컬렉션에 등록하여 전 세계 사람들과 공유하는 기회를 갖게 됩니다.\n\"Hugging Face KREW Hackathon 2023\" (collection)\n    ├── 참가자프로젝트(1) (space)\n    ├── ...\n    ├── 참가자프로젝트(n-1) (model)\n    └── 참가자프로젝트(n) (dataset)\n\nHugging Face 가입\n시작하기 전에, hf.co/join 링크를 통해 Hugging Face에 가입하시고, 전 세계적인 커뮤니티의 일원이 되어 주세요. 간단한 회원 가입 과정을 거치면 됩니다.\n\n\n\n\n\n\n\nPseudo Lab 기여자 등록\n회원 가입을 마친 후, 기여자 등록 페이지로 이동하여 ’Pseudo Lab’의 프로젝트에 기여할 수 있는 권한을 얻으세요. 여러분의 아이디어가 빛을 발할 수 있는 공간을 제공합니다.\n\n\n\n\n\n\n\n바로 프로젝트 생성해보기\nPseudo Lab 조직 내에서 New 버튼을 눌러 새로운 Space를 시험삼아 만들어보세요. 여기서 Space name은 여러분의 프로젝트를 대표할 이름을 사용해 주세요. 라이선스 선택 부분에서는 “Apache 2.0 License”를 추천합니다.\n\n\n\n\n\n\n\n\n\n\n프로젝트를 실제로 작동시킬 설정도 중요합니다. SDK 옵션에서는 특히 초보자분들께 Gradio를 추천드려요. 사용하기 쉽고, 다양한 예제를 통해 빠르게 배울 수 있습니다. 그리고 하드웨어는 기본적으로 제공되는 무료 CPU 자원을 사용하되, 필요에 따라 추가적인 GPU 자원을 요청할 수도 있어요. 하지만, 모든 참가자가 자원을 공유해야 하니 꼭 필요한 경우에만 요청해 주세요!\n\n\n\n\n\n\n\n\n\n\n\n\n고급 GPU 하드웨어 자원을 사용하면 해커톤 전체 Credit이 소모됩니다.\n다른 참가자를 배려한 효율적인 사용 부탁드립니다.\n\n\n\n\n\n\n기존 프로젝트 가져오기\n개인 스페이스에서 프로젝트를 먼저 시작한 경우에도 걱정하지 마세요. 스페이스를 복제한 후, 소유권을 pseudolab으로 변경하면 동일하게 리더보드에서 보실 수 있습니다.\n \n\n\nREADME 파일 작성하기\n프로젝트의 README 파일은 누구나 프로젝트를 쉽게 이해할 수 있도록 상세하게 작성해야 해요. 프로젝트의 목적, 사용된 기술, 주요 변경 사항 등을 포함하여, 충분한 정보를 제공해 주세요. 다른 참가자들이 더욱 더 흥미롭게 즐길 수 있을 거에요!\n마지막으로 pinned 항목을 false로 설정해주세요. 공정한 평가를 위함이며, 홍보는 (sns 등) 다른 채널을 통해 해주시길 바랍니다.\n\n\nREADME.md\n\ntitle: 'My Project'\nemoji: 🌌\ncolorFrom: purple\ncolorTo: yellow\nsdk: gradio\nsdk_version: 3.36.1\napp_file: app.py\npinned: false"
  },
  {
    "objectID": "presentations/01/index.html",
    "href": "presentations/01/index.html",
    "title": "Hugging Face 도전기",
    "section": "",
    "text": "See in English"
  },
  {
    "objectID": "presentations/00/index.html",
    "href": "presentations/00/index.html",
    "title": "Hugging Face 튜토리얼",
    "section": "",
    "text": "See in English"
  },
  {
    "objectID": "presentations/05/index.html",
    "href": "presentations/05/index.html",
    "title": "SonGPT",
    "section": "",
    "text": "See in English"
  },
  {
    "objectID": "tutorials-templates/index.html",
    "href": "tutorials-templates/index.html",
    "title": "Notebooks for live-coding",
    "section": "",
    "text": "See in English\nTemplates of all tutorials — .ipynb notebooks with code removed — are available at:\nhttps://github.com/NASA-Openscapes/2021-Cloud-Hackathon/tree/main/tutorials-templates.\nPlease open these tutorial templates in our JupyterHub to follow along and live-code with the tutorial lead."
  },
  {
    "objectID": "intro/code-of-conduct.html",
    "href": "intro/code-of-conduct.html",
    "title": "2023 일상에서의 AI 해커톤",
    "section": "",
    "text": "See in English"
  }
]